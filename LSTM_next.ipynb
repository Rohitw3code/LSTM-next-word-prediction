{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "hNs45lJqlBhq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding,LSTM,Dense"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"story.txt\",\"r\",encoding=\"utf-8\") as myfile:\n",
        "  mytext = myfile.read()"
      ],
      "metadata": {
        "id": "Vd54K1nTl17_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mytext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "tArd5JqtmbnL",
        "outputId": "b0bf053e-2f0b-4919-c489-39cf81ae5e55"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Learning to program is hard. It\\'s difficult to know where to start, but it\\'s even more challenging to know what to do after you\\'ve learned the basics. Jumping into a developer bootcamp for three months isn\\'t enough.\\n\\nThe following stories from successful developers (some of them self-taught) show you what it takes to join their ranks, highlighting how the authors learned programming and landed their first job, as well as chronicling people\\'s moves from novice professional developer to exceptional software engineer.\\n\\n\\'How I became a developer\\' stories\\nPeople can become developers from almost any background, whether they started programming at an early age and majored in computer science (CS), or they taught themselves how to code later in life. These stories focus on how the authors went from being beginning coders to getting that first job.\\n\\nMy journey to becoming a web developer from scratch without a CS degree\\nThis article by Sergei Garcia is packed with excellent resources for anyone learning to become a front-end developer. He shares the technical details of his journey, along with almost every resource that he used. Garcia also includes the story of his first job and how he kept improving his skills after he landed it. Finally, he concludes with some advice, pitfalls, and an outline reiterating all of the learning materials that helped him, in the order that they should be approached.\\n\\nHow I went from mopping floors at a tanning salon to becoming a software developer\\nNnenna Ndukwe describes a pivotal time in her life when she was out of funds to pursue a college degree and how she made the decision to leave town and learn programming. In addition to anecdotes, the story shares five strategies and tools that she used to build her programming skills. Her account shows us how important it is to struggle through the process of introspection and rediscover your passions.\\n\\n\"Here I was, realizing that I was returning to my original nerdy self, but approaching and engaging with technology from a different angle.\" —Nnenna Ndukwe\\n\\nHow I learned to program in 10 years\\nJulia Evans is an extremely influential person in the software and operations engineering community. Her blog and Twitter account are super-popular, but the one thing you should know about Evans is that she has an always-learning mindset. While this post isn\\'t saying that you need 10 years before you can become a programmer, it is saying that you should always feel as if you\\'re \"becoming\" a programmer—even after you\\'ve gotten your first job. And while the years when she could devote more time to programming were the most beneficial, the years of less frequent hobby work in grade school and college helped build an important foundation. The bulk of this goes over the most interesting programming milestones in her life.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mytokenizer = Tokenizer()\n",
        "mytokenizer.fit_on_texts([mytext])\n",
        "total_words = len(mytokenizer.word_index)+1"
      ],
      "metadata": {
        "id": "gZYSSkrLmc1K"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mytokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0o_Mn9FmnSqJ",
        "outputId": "1b5052a8-0556-42e7-b615-db2d955a705d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'to': 1,\n",
              " 'the': 2,\n",
              " 'and': 3,\n",
              " 'a': 4,\n",
              " 'in': 5,\n",
              " 'that': 6,\n",
              " 'of': 7,\n",
              " 'how': 8,\n",
              " 'from': 9,\n",
              " 'is': 10,\n",
              " 'programming': 11,\n",
              " 'developer': 12,\n",
              " 'you': 13,\n",
              " 'i': 14,\n",
              " 'an': 15,\n",
              " 'he': 16,\n",
              " 'her': 17,\n",
              " 'she': 18,\n",
              " 'learning': 19,\n",
              " 'it': 20,\n",
              " 'first': 21,\n",
              " 'job': 22,\n",
              " 'with': 23,\n",
              " 'years': 24,\n",
              " 'know': 25,\n",
              " 'but': 26,\n",
              " 'after': 27,\n",
              " 'learned': 28,\n",
              " 'stories': 29,\n",
              " 'as': 30,\n",
              " 'software': 31,\n",
              " 'become': 32,\n",
              " 'they': 33,\n",
              " 'life': 34,\n",
              " 'becoming': 35,\n",
              " 'this': 36,\n",
              " 'his': 37,\n",
              " 'should': 38,\n",
              " 'was': 39,\n",
              " 'program': 40,\n",
              " \"it's\": 41,\n",
              " 'more': 42,\n",
              " 'what': 43,\n",
              " \"you've\": 44,\n",
              " 'for': 45,\n",
              " \"isn't\": 46,\n",
              " 'developers': 47,\n",
              " 'some': 48,\n",
              " 'self': 49,\n",
              " 'taught': 50,\n",
              " 'their': 51,\n",
              " 'authors': 52,\n",
              " 'landed': 53,\n",
              " 'can': 54,\n",
              " 'almost': 55,\n",
              " 'at': 56,\n",
              " 'cs': 57,\n",
              " 'went': 58,\n",
              " 'my': 59,\n",
              " 'journey': 60,\n",
              " 'degree': 61,\n",
              " 'garcia': 62,\n",
              " 'shares': 63,\n",
              " 'used': 64,\n",
              " 'story': 65,\n",
              " 'skills': 66,\n",
              " 'helped': 67,\n",
              " 'ndukwe': 68,\n",
              " 'time': 69,\n",
              " 'when': 70,\n",
              " 'college': 71,\n",
              " 'build': 72,\n",
              " 'account': 73,\n",
              " 'important': 74,\n",
              " 'your': 75,\n",
              " '10': 76,\n",
              " 'evans': 77,\n",
              " 'always': 78,\n",
              " 'while': 79,\n",
              " 'saying': 80,\n",
              " 'most': 81,\n",
              " 'hard': 82,\n",
              " 'difficult': 83,\n",
              " 'where': 84,\n",
              " 'start': 85,\n",
              " 'even': 86,\n",
              " 'challenging': 87,\n",
              " 'do': 88,\n",
              " 'basics': 89,\n",
              " 'jumping': 90,\n",
              " 'into': 91,\n",
              " 'bootcamp': 92,\n",
              " 'three': 93,\n",
              " 'months': 94,\n",
              " 'enough': 95,\n",
              " 'following': 96,\n",
              " 'successful': 97,\n",
              " 'them': 98,\n",
              " 'show': 99,\n",
              " 'takes': 100,\n",
              " 'join': 101,\n",
              " 'ranks': 102,\n",
              " 'highlighting': 103,\n",
              " 'well': 104,\n",
              " 'chronicling': 105,\n",
              " \"people's\": 106,\n",
              " 'moves': 107,\n",
              " 'novice': 108,\n",
              " 'professional': 109,\n",
              " 'exceptional': 110,\n",
              " 'engineer': 111,\n",
              " \"'how\": 112,\n",
              " 'became': 113,\n",
              " \"developer'\": 114,\n",
              " 'people': 115,\n",
              " 'any': 116,\n",
              " 'background': 117,\n",
              " 'whether': 118,\n",
              " 'started': 119,\n",
              " 'early': 120,\n",
              " 'age': 121,\n",
              " 'majored': 122,\n",
              " 'computer': 123,\n",
              " 'science': 124,\n",
              " 'or': 125,\n",
              " 'themselves': 126,\n",
              " 'code': 127,\n",
              " 'later': 128,\n",
              " 'these': 129,\n",
              " 'focus': 130,\n",
              " 'on': 131,\n",
              " 'being': 132,\n",
              " 'beginning': 133,\n",
              " 'coders': 134,\n",
              " 'getting': 135,\n",
              " 'web': 136,\n",
              " 'scratch': 137,\n",
              " 'without': 138,\n",
              " 'article': 139,\n",
              " 'by': 140,\n",
              " 'sergei': 141,\n",
              " 'packed': 142,\n",
              " 'excellent': 143,\n",
              " 'resources': 144,\n",
              " 'anyone': 145,\n",
              " 'front': 146,\n",
              " 'end': 147,\n",
              " 'technical': 148,\n",
              " 'details': 149,\n",
              " 'along': 150,\n",
              " 'every': 151,\n",
              " 'resource': 152,\n",
              " 'also': 153,\n",
              " 'includes': 154,\n",
              " 'kept': 155,\n",
              " 'improving': 156,\n",
              " 'finally': 157,\n",
              " 'concludes': 158,\n",
              " 'advice': 159,\n",
              " 'pitfalls': 160,\n",
              " 'outline': 161,\n",
              " 'reiterating': 162,\n",
              " 'all': 163,\n",
              " 'materials': 164,\n",
              " 'him': 165,\n",
              " 'order': 166,\n",
              " 'be': 167,\n",
              " 'approached': 168,\n",
              " 'mopping': 169,\n",
              " 'floors': 170,\n",
              " 'tanning': 171,\n",
              " 'salon': 172,\n",
              " 'nnenna': 173,\n",
              " 'describes': 174,\n",
              " 'pivotal': 175,\n",
              " 'out': 176,\n",
              " 'funds': 177,\n",
              " 'pursue': 178,\n",
              " 'made': 179,\n",
              " 'decision': 180,\n",
              " 'leave': 181,\n",
              " 'town': 182,\n",
              " 'learn': 183,\n",
              " 'addition': 184,\n",
              " 'anecdotes': 185,\n",
              " 'five': 186,\n",
              " 'strategies': 187,\n",
              " 'tools': 188,\n",
              " 'shows': 189,\n",
              " 'us': 190,\n",
              " 'struggle': 191,\n",
              " 'through': 192,\n",
              " 'process': 193,\n",
              " 'introspection': 194,\n",
              " 'rediscover': 195,\n",
              " 'passions': 196,\n",
              " 'here': 197,\n",
              " 'realizing': 198,\n",
              " 'returning': 199,\n",
              " 'original': 200,\n",
              " 'nerdy': 201,\n",
              " 'approaching': 202,\n",
              " 'engaging': 203,\n",
              " 'technology': 204,\n",
              " 'different': 205,\n",
              " 'angle': 206,\n",
              " '—nnenna': 207,\n",
              " 'julia': 208,\n",
              " 'extremely': 209,\n",
              " 'influential': 210,\n",
              " 'person': 211,\n",
              " 'operations': 212,\n",
              " 'engineering': 213,\n",
              " 'community': 214,\n",
              " 'blog': 215,\n",
              " 'twitter': 216,\n",
              " 'are': 217,\n",
              " 'super': 218,\n",
              " 'popular': 219,\n",
              " 'one': 220,\n",
              " 'thing': 221,\n",
              " 'about': 222,\n",
              " 'has': 223,\n",
              " 'mindset': 224,\n",
              " 'post': 225,\n",
              " 'need': 226,\n",
              " 'before': 227,\n",
              " 'programmer': 228,\n",
              " 'feel': 229,\n",
              " 'if': 230,\n",
              " \"you're\": 231,\n",
              " 'programmer—even': 232,\n",
              " 'gotten': 233,\n",
              " 'could': 234,\n",
              " 'devote': 235,\n",
              " 'were': 236,\n",
              " 'beneficial': 237,\n",
              " 'less': 238,\n",
              " 'frequent': 239,\n",
              " 'hobby': 240,\n",
              " 'work': 241,\n",
              " 'grade': 242,\n",
              " 'school': 243,\n",
              " 'foundation': 244,\n",
              " 'bulk': 245,\n",
              " 'goes': 246,\n",
              " 'over': 247,\n",
              " 'interesting': 248,\n",
              " 'milestones': 249}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_input_sequences = []\n",
        "for line in mytext.split('\\n'):\n",
        "    #print(line)\n",
        "    token_list = mytokenizer.texts_to_sequences([line])[0]\n",
        "    #print(token_list)\n",
        "    for i in range(1, len(token_list)):\n",
        "        my_n_gram_sequence = token_list[:i+1]\n",
        "        #print(my_n_gram_sequence)\n",
        "        my_input_sequences.append(my_n_gram_sequence)\n",
        "        #print(input_sequences)"
      ],
      "metadata": {
        "id": "dmcl-VLFnYzS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_sequence_len = max([len(seq) for seq in my_input_sequences])\n",
        "input_sequences = np.array(pad_sequences(my_input_sequences, maxlen=max_sequence_len))"
      ],
      "metadata": {
        "id": "aZOpiD2hnyyE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYxjWIB_o5Ic",
        "outputId": "a7c21044-5901-4fe2-cb9f-678537280b7a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "       19,  1, 40], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = input_sequences[:, :-1]\n",
        "y = input_sequences[:, -1]"
      ],
      "metadata": {
        "id": "4XXeELqio_rT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTfUWxwJpNS9",
        "outputId": "b0c98db7-d7e1-4ca4-dbe5-05ba16bbce0e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "       19,  1], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1Qr--chpQp1",
        "outputId": "adc1715a-7b68-4495-eee0-01d1ffd6a450"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ih9-qcWWpULC",
        "outputId": "5c1c6753-1b23-4889-af67-74b86a6f1d45"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  1,  40,  10,  82,  41,  83,   1,  25,  84,   1,  85,  26,  41,\n",
              "        86,  42,  87,   1,  25,  43,   1,  88,  27,  44,  28,   2,  89,\n",
              "        90,  91,   4,  12,  92,  45,  93,  94,  46,  95,  96,  29,   9,\n",
              "        97,  47,  48,   7,  98,  49,  50,  99,  13,  43,  20, 100,   1,\n",
              "       101,  51, 102, 103,   8,   2,  52,  28,  11,   3,  53,  51,  21,\n",
              "        22,  30, 104,  30, 105, 106, 107,   9, 108, 109,  12,   1, 110,\n",
              "        31, 111,  14, 113,   4, 114,  29,  54,  32,  47,   9,  55, 116,\n",
              "       117, 118,  33, 119,  11,  56,  15, 120, 121,   3, 122,   5, 123,\n",
              "       124,  57, 125,  33,  50, 126,   8,   1, 127, 128,   5,  34, 129,\n",
              "        29, 130, 131,   8,   2,  52,  58,   9, 132, 133, 134,   1, 135,\n",
              "         6,  21,  22,  60,   1,  35,   4, 136,  12,   9, 137, 138,   4,\n",
              "        57,  61, 139, 140, 141,  62,  10, 142,  23, 143, 144,  45, 145,\n",
              "        19,   1,  32,   4, 146, 147,  12,  16,  63,   2, 148, 149,   7,\n",
              "        37,  60, 150,  23,  55, 151, 152,   6,  16,  64,  62, 153, 154,\n",
              "         2,  65,   7,  37,  21,  22,   3,   8,  16, 155, 156,  37,  66,\n",
              "        27,  16,  53,  20, 157,  16, 158,  23,  48, 159, 160,   3,  15,\n",
              "       161, 162, 163,   7,   2,  19, 164,   6,  67, 165,   5,   2, 166,\n",
              "         6,  33,  38, 167, 168,  14,  58,   9, 169, 170,  56,   4, 171,\n",
              "       172,   1,  35,   4,  31,  12,  68, 174,   4, 175,  69,   5,  17,\n",
              "        34,  70,  18,  39, 176,   7, 177,   1, 178,   4,  71,  61,   3,\n",
              "         8,  18, 179,   2, 180,   1, 181, 182,   3, 183,  11,   5, 184,\n",
              "         1, 185,   2,  65,  63, 186, 187,   3, 188,   6,  18,  64,   1,\n",
              "        72,  17,  11,  66,  17,  73, 189, 190,   8,  74,  20,  10,   1,\n",
              "       191, 192,   2, 193,   7, 194,   3, 195,  75, 196,  14,  39, 198,\n",
              "         6,  14,  39, 199,   1,  59, 200, 201,  49,  26, 202,   3, 203,\n",
              "        23, 204,   9,   4, 205, 206, 207,  68,  14,  28,   1,  40,   5,\n",
              "        76,  24,  77,  10,  15, 209, 210, 211,   5,   2,  31,   3, 212,\n",
              "       213, 214,  17, 215,   3, 216,  73, 217, 218, 219,  26,   2, 220,\n",
              "       221,  13,  38,  25, 222,  77,  10,   6,  18, 223,  15,  78,  19,\n",
              "       224,  79,  36, 225,  46,  80,   6,  13, 226,  76,  24, 227,  13,\n",
              "        54,  32,   4, 228,  20,  10,  80,   6,  13,  38,  78, 229,  30,\n",
              "       230, 231,  35,   4, 232,  27,  44, 233,  75,  21,  22,   3,  79,\n",
              "         2,  24,  70,  18, 234, 235,  42,  69,   1,  11, 236,   2,  81,\n",
              "       237,   2,  24,   7, 238, 239, 240, 241,   5, 242, 243,   3,  71,\n",
              "        67,  72,  15,  74, 244,   2, 245,   7,  36, 246, 247,   2,  81,\n",
              "       248,  11, 249,   5,  17,  34], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.array(tf.keras.utils.to_categorical(y, num_classes=total_words))"
      ],
      "metadata": {
        "id": "3lZ33LHupgWA"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4W-_qexPpkVa",
        "outputId": "0d1aafc4-cf1a-4e34-8c1d-7c9ded0666a1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NraQHitXpyMQ",
        "outputId": "4d2735b7-6e96-4565-8486-27751220b6af"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 121, 100)          25000     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 150)               150600    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 250)               37750     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 213,350\n",
            "Trainable params: 213,350\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=100, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bkkZYu9qb3i",
        "outputId": "f6007ad5-467a-4dd3-962d-5eb161f13197"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "15/15 [==============================] - 6s 227ms/step - loss: 5.5171 - accuracy: 0.0174\n",
            "Epoch 2/100\n",
            "15/15 [==============================] - 4s 276ms/step - loss: 5.3869 - accuracy: 0.0434\n",
            "Epoch 3/100\n",
            "15/15 [==============================] - 4s 270ms/step - loss: 5.2282 - accuracy: 0.0434\n",
            "Epoch 4/100\n",
            "15/15 [==============================] - 3s 229ms/step - loss: 5.1574 - accuracy: 0.0434\n",
            "Epoch 5/100\n",
            "15/15 [==============================] - 3s 228ms/step - loss: 5.1066 - accuracy: 0.0542\n",
            "Epoch 6/100\n",
            "15/15 [==============================] - 5s 337ms/step - loss: 5.0399 - accuracy: 0.0542\n",
            "Epoch 7/100\n",
            "15/15 [==============================] - 4s 234ms/step - loss: 4.9789 - accuracy: 0.0564\n",
            "Epoch 8/100\n",
            "15/15 [==============================] - 3s 228ms/step - loss: 4.9035 - accuracy: 0.0542\n",
            "Epoch 9/100\n",
            "15/15 [==============================] - 5s 336ms/step - loss: 4.8368 - accuracy: 0.0651\n",
            "Epoch 10/100\n",
            "15/15 [==============================] - 3s 230ms/step - loss: 4.7477 - accuracy: 0.0672\n",
            "Epoch 11/100\n",
            "15/15 [==============================] - 3s 233ms/step - loss: 4.6601 - accuracy: 0.0672\n",
            "Epoch 12/100\n",
            "15/15 [==============================] - 4s 273ms/step - loss: 4.5589 - accuracy: 0.0651\n",
            "Epoch 13/100\n",
            "15/15 [==============================] - 4s 280ms/step - loss: 4.4525 - accuracy: 0.0803\n",
            "Epoch 14/100\n",
            "15/15 [==============================] - 3s 228ms/step - loss: 4.3986 - accuracy: 0.0759\n",
            "Epoch 15/100\n",
            "15/15 [==============================] - 4s 235ms/step - loss: 4.2806 - accuracy: 0.0889\n",
            "Epoch 16/100\n",
            "15/15 [==============================] - 5s 348ms/step - loss: 4.1287 - accuracy: 0.0911\n",
            "Epoch 17/100\n",
            "15/15 [==============================] - 4s 236ms/step - loss: 3.9678 - accuracy: 0.1215\n",
            "Epoch 18/100\n",
            "15/15 [==============================] - 4s 237ms/step - loss: 3.8206 - accuracy: 0.1236\n",
            "Epoch 19/100\n",
            "15/15 [==============================] - 5s 342ms/step - loss: 3.6795 - accuracy: 0.1627\n",
            "Epoch 20/100\n",
            "15/15 [==============================] - 3s 230ms/step - loss: 3.5244 - accuracy: 0.1562\n",
            "Epoch 21/100\n",
            "15/15 [==============================] - 3s 231ms/step - loss: 3.3702 - accuracy: 0.1800\n",
            "Epoch 22/100\n",
            "15/15 [==============================] - 4s 302ms/step - loss: 3.1984 - accuracy: 0.2148\n",
            "Epoch 23/100\n",
            "15/15 [==============================] - 4s 246ms/step - loss: 3.0450 - accuracy: 0.2538\n",
            "Epoch 24/100\n",
            "15/15 [==============================] - 3s 229ms/step - loss: 2.8993 - accuracy: 0.2885\n",
            "Epoch 25/100\n",
            "15/15 [==============================] - 4s 235ms/step - loss: 2.7418 - accuracy: 0.3406\n",
            "Epoch 26/100\n",
            "15/15 [==============================] - 5s 322ms/step - loss: 2.6003 - accuracy: 0.3666\n",
            "Epoch 27/100\n",
            "15/15 [==============================] - 3s 231ms/step - loss: 2.4636 - accuracy: 0.4382\n",
            "Epoch 28/100\n",
            "15/15 [==============================] - 4s 234ms/step - loss: 2.3339 - accuracy: 0.4317\n",
            "Epoch 29/100\n",
            "15/15 [==============================] - 6s 408ms/step - loss: 2.2014 - accuracy: 0.5054\n",
            "Epoch 30/100\n",
            "15/15 [==============================] - 4s 264ms/step - loss: 2.0782 - accuracy: 0.5662\n",
            "Epoch 31/100\n",
            "15/15 [==============================] - 3s 227ms/step - loss: 1.9647 - accuracy: 0.5879\n",
            "Epoch 32/100\n",
            "15/15 [==============================] - 5s 339ms/step - loss: 1.8526 - accuracy: 0.6551\n",
            "Epoch 33/100\n",
            "15/15 [==============================] - 4s 233ms/step - loss: 1.7564 - accuracy: 0.6790\n",
            "Epoch 34/100\n",
            "15/15 [==============================] - 3s 231ms/step - loss: 1.6529 - accuracy: 0.7505\n",
            "Epoch 35/100\n",
            "15/15 [==============================] - 5s 305ms/step - loss: 1.5534 - accuracy: 0.7549\n",
            "Epoch 36/100\n",
            "15/15 [==============================] - 4s 244ms/step - loss: 1.4685 - accuracy: 0.7918\n",
            "Epoch 37/100\n",
            "15/15 [==============================] - 3s 231ms/step - loss: 1.3895 - accuracy: 0.8330\n",
            "Epoch 38/100\n",
            "15/15 [==============================] - 3s 233ms/step - loss: 1.3108 - accuracy: 0.8525\n",
            "Epoch 39/100\n",
            "15/15 [==============================] - 5s 331ms/step - loss: 1.2344 - accuracy: 0.8807\n",
            "Epoch 40/100\n",
            "15/15 [==============================] - 3s 230ms/step - loss: 1.1629 - accuracy: 0.8807\n",
            "Epoch 41/100\n",
            "15/15 [==============================] - 3s 231ms/step - loss: 1.0953 - accuracy: 0.9024\n",
            "Epoch 42/100\n",
            "15/15 [==============================] - 5s 336ms/step - loss: 1.0364 - accuracy: 0.9262\n",
            "Epoch 43/100\n",
            "15/15 [==============================] - 3s 231ms/step - loss: 0.9764 - accuracy: 0.9501\n",
            "Epoch 44/100\n",
            "15/15 [==============================] - 3s 231ms/step - loss: 0.9297 - accuracy: 0.9501\n",
            "Epoch 45/100\n",
            "15/15 [==============================] - 5s 305ms/step - loss: 0.8724 - accuracy: 0.9696\n",
            "Epoch 46/100\n",
            "15/15 [==============================] - 4s 240ms/step - loss: 0.8221 - accuracy: 0.9805\n",
            "Epoch 47/100\n",
            "15/15 [==============================] - 3s 231ms/step - loss: 0.7802 - accuracy: 0.9805\n",
            "Epoch 48/100\n",
            "15/15 [==============================] - 3s 229ms/step - loss: 0.7301 - accuracy: 0.9826\n",
            "Epoch 49/100\n",
            "15/15 [==============================] - 5s 328ms/step - loss: 0.6950 - accuracy: 0.9892\n",
            "Epoch 50/100\n",
            "15/15 [==============================] - 3s 225ms/step - loss: 0.6530 - accuracy: 0.9913\n",
            "Epoch 51/100\n",
            "15/15 [==============================] - 4s 233ms/step - loss: 0.6177 - accuracy: 0.9913\n",
            "Epoch 52/100\n",
            "15/15 [==============================] - 5s 343ms/step - loss: 0.5861 - accuracy: 0.9957\n",
            "Epoch 53/100\n",
            "15/15 [==============================] - 3s 232ms/step - loss: 0.5577 - accuracy: 0.9935\n",
            "Epoch 54/100\n",
            "15/15 [==============================] - 3s 229ms/step - loss: 0.5303 - accuracy: 0.9913\n",
            "Epoch 55/100\n",
            "15/15 [==============================] - 5s 307ms/step - loss: 0.5060 - accuracy: 0.9957\n",
            "Epoch 56/100\n",
            "15/15 [==============================] - 4s 250ms/step - loss: 0.4724 - accuracy: 0.9957\n",
            "Epoch 57/100\n",
            "15/15 [==============================] - 3s 232ms/step - loss: 0.4490 - accuracy: 0.9957\n",
            "Epoch 58/100\n",
            "15/15 [==============================] - 4s 241ms/step - loss: 0.4254 - accuracy: 0.9957\n",
            "Epoch 59/100\n",
            "15/15 [==============================] - 5s 316ms/step - loss: 0.4045 - accuracy: 0.9957\n",
            "Epoch 60/100\n",
            "15/15 [==============================] - 4s 235ms/step - loss: 0.3869 - accuracy: 0.9957\n",
            "Epoch 61/100\n",
            "15/15 [==============================] - 4s 233ms/step - loss: 0.3646 - accuracy: 0.9978\n",
            "Epoch 62/100\n",
            "15/15 [==============================] - 5s 334ms/step - loss: 0.3476 - accuracy: 0.9978\n",
            "Epoch 63/100\n",
            "15/15 [==============================] - 3s 232ms/step - loss: 0.3312 - accuracy: 0.9957\n",
            "Epoch 64/100\n",
            "15/15 [==============================] - 3s 231ms/step - loss: 0.3167 - accuracy: 0.9978\n",
            "Epoch 65/100\n",
            "15/15 [==============================] - 5s 325ms/step - loss: 0.3036 - accuracy: 0.9957\n",
            "Epoch 66/100\n",
            "15/15 [==============================] - 4s 230ms/step - loss: 0.2897 - accuracy: 0.9978\n",
            "Epoch 67/100\n",
            "15/15 [==============================] - 3s 225ms/step - loss: 0.2747 - accuracy: 0.9978\n",
            "Epoch 68/100\n",
            "15/15 [==============================] - 4s 234ms/step - loss: 0.2621 - accuracy: 0.9957\n",
            "Epoch 69/100\n",
            "15/15 [==============================] - 5s 310ms/step - loss: 0.2507 - accuracy: 0.9978\n",
            "Epoch 70/100\n",
            "15/15 [==============================] - 3s 232ms/step - loss: 0.2404 - accuracy: 0.9978\n",
            "Epoch 71/100\n",
            "15/15 [==============================] - 3s 227ms/step - loss: 0.2310 - accuracy: 0.9957\n",
            "Epoch 72/100\n",
            "15/15 [==============================] - 5s 332ms/step - loss: 0.2227 - accuracy: 0.9978\n",
            "Epoch 73/100\n",
            "15/15 [==============================] - 3s 224ms/step - loss: 0.2144 - accuracy: 0.9978\n",
            "Epoch 74/100\n",
            "15/15 [==============================] - 4s 233ms/step - loss: 0.2059 - accuracy: 0.9957\n",
            "Epoch 75/100\n",
            "15/15 [==============================] - 6s 429ms/step - loss: 0.1972 - accuracy: 0.9957\n",
            "Epoch 76/100\n",
            "15/15 [==============================] - 3s 226ms/step - loss: 0.1892 - accuracy: 0.9957\n",
            "Epoch 77/100\n",
            "15/15 [==============================] - 3s 227ms/step - loss: 0.1823 - accuracy: 0.9978\n",
            "Epoch 78/100\n",
            "15/15 [==============================] - 4s 260ms/step - loss: 0.1750 - accuracy: 0.9957\n",
            "Epoch 79/100\n",
            "15/15 [==============================] - 4s 281ms/step - loss: 0.1696 - accuracy: 0.9957\n",
            "Epoch 80/100\n",
            "15/15 [==============================] - 3s 228ms/step - loss: 0.1632 - accuracy: 0.9978\n",
            "Epoch 81/100\n",
            "15/15 [==============================] - 4s 235ms/step - loss: 0.1561 - accuracy: 0.9978\n",
            "Epoch 82/100\n",
            "15/15 [==============================] - 5s 337ms/step - loss: 0.1504 - accuracy: 0.9978\n",
            "Epoch 83/100\n",
            "15/15 [==============================] - 3s 229ms/step - loss: 0.1451 - accuracy: 0.9978\n",
            "Epoch 84/100\n",
            "15/15 [==============================] - 3s 222ms/step - loss: 0.1405 - accuracy: 0.9957\n",
            "Epoch 85/100\n",
            "15/15 [==============================] - 5s 333ms/step - loss: 0.1357 - accuracy: 0.9978\n",
            "Epoch 86/100\n",
            "15/15 [==============================] - 3s 227ms/step - loss: 0.1316 - accuracy: 0.9957\n",
            "Epoch 87/100\n",
            "15/15 [==============================] - 3s 228ms/step - loss: 0.1278 - accuracy: 0.9978\n",
            "Epoch 88/100\n",
            "15/15 [==============================] - 4s 247ms/step - loss: 0.1232 - accuracy: 0.9978\n",
            "Epoch 89/100\n",
            "15/15 [==============================] - 5s 296ms/step - loss: 0.1193 - accuracy: 0.9957\n",
            "Epoch 90/100\n",
            "15/15 [==============================] - 3s 228ms/step - loss: 0.1157 - accuracy: 0.9978\n",
            "Epoch 91/100\n",
            "15/15 [==============================] - 3s 228ms/step - loss: 0.1122 - accuracy: 0.9978\n",
            "Epoch 92/100\n",
            "15/15 [==============================] - 5s 335ms/step - loss: 0.1096 - accuracy: 0.9978\n",
            "Epoch 93/100\n",
            "15/15 [==============================] - 3s 224ms/step - loss: 0.1056 - accuracy: 0.9978\n",
            "Epoch 94/100\n",
            "15/15 [==============================] - 3s 227ms/step - loss: 0.1029 - accuracy: 0.9957\n",
            "Epoch 95/100\n",
            "15/15 [==============================] - 5s 315ms/step - loss: 0.0996 - accuracy: 0.9978\n",
            "Epoch 96/100\n",
            "15/15 [==============================] - 4s 230ms/step - loss: 0.0967 - accuracy: 0.9978\n",
            "Epoch 97/100\n",
            "15/15 [==============================] - 3s 224ms/step - loss: 0.0944 - accuracy: 0.9957\n",
            "Epoch 98/100\n",
            "15/15 [==============================] - 3s 228ms/step - loss: 0.0922 - accuracy: 0.9978\n",
            "Epoch 99/100\n",
            "15/15 [==============================] - 5s 322ms/step - loss: 0.0897 - accuracy: 0.9957\n",
            "Epoch 100/100\n",
            "15/15 [==============================] - 3s 225ms/step - loss: 0.0868 - accuracy: 0.9978\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x79c1944b51b0>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"How I became a developer\"\n",
        "predict_next_words= 20\n",
        "\n",
        "for _ in range(predict_next_words):\n",
        "    token_list = mytokenizer.texts_to_sequences([input_text])[0]\n",
        "    # print(token_list)\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "    output_word = \"\"\n",
        "    for word, index in mytokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "    input_text += \" \" + output_word\n",
        "\n",
        "print(input_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvK__i4wqvCO",
        "outputId": "5521fb1f-645a-45e1-f5d8-25bcd4dc8e45"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "How I became a developer from mopping floors at a tanning salon to becoming a software developer from a degree and how she made the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8DbJErXBscGQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}